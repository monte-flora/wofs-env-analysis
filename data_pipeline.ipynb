{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e1d0bd",
   "metadata": {},
   "source": [
    "## Data Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conda requirement: python 3.8, xarray, numpy, scipy, numba, monte_python, scikit-explain\n",
    "\n",
    "# Import required libraries \n",
    "from glob import glob \n",
    "from os.path import join\n",
    "\n",
    "# Xarray is a great library for processing n-dimensional arrays! \n",
    "import xarray as xr \n",
    "import numpy as np \n",
    "import numba as nb \n",
    "from scipy.ndimage import maximum_filter\n",
    "from skexplain.common.multiprocessing_utils import run_parallel, to_iterator \n",
    "\n",
    "# Download https://github.com/WarnOnForecast/wofs_ml_severe\n",
    "# change your system path.\n",
    "import sys\n",
    "sys.path.append('/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "\n",
    "# To use MontePython, clone it to your local directory\n",
    "# and then run `python setup.py install` \n",
    "import monte_python \n",
    "from monte_python.object_identification import quantize_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62efdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constant variables (like base paths).\n",
    "BASE_DATA_PATH = '/work/mflora/SummaryFiles/'\n",
    "OUTDIR = '/work/mflora/tmp'\n",
    "ENSEMBLE_SIZE = 18 \n",
    "PATCH_SIZE_RADIUS = 20 # on a 3-km grid, this would be 40 x 40 patch or 120 x 120 km grid. \n",
    "PROB_THRESH = 4/18 \n",
    "N_JOBS = 1\n",
    "SVR_VARIABLES = ['srh_0to1', 'cape_ml', 'cin_ml']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38656c",
   "metadata": {},
   "source": [
    "### Questions \n",
    "\n",
    "1. Do we limit up to a lead time of an hour? \n",
    "\n",
    "For a instant NMEP valid at t=x, we want the t-15, t-30, t-45, t-60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(date, init_time, time_index):\n",
    "    \"\"\"Load the appropriate WoFS summary files\"\"\"\n",
    "    \n",
    "    assert time_index>=12 \n",
    "    \n",
    "    # TODO: ENS has to be at a minimum of an hour. \n",
    "    # to prevent sampling too close to initialization. \n",
    "\n",
    "    file_path_svr = glob(join(BASE_DATA_PATH, date, init_time, f'wofs_SVR_{time_index}*'))[0]\n",
    "    file_path_ens = glob(join(BASE_DATA_PATH, date, init_time, f'wofs_ENS_{time_index}*'))[0]\n",
    "\n",
    "    ds_ens = xr.load_dataset(file_path_ens, decode_times=False)\n",
    "    ds_svr = xr.load_dataset(file_path_svr, decode_times=False)\n",
    "    \n",
    "    return ds_ens, ds_svr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(fname, dataset):\n",
    "    \"\"\" saves xarray dataset to netcdf using compression \"\"\"\n",
    "    comp = dict(zlib=True, complevel=5)\n",
    "    encoding = {var: comp for var in dataset.data_vars}\n",
    "    #os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    dataset.to_netcdf( path = fname, encoding=encoding )\n",
    "    dataset.close( )\n",
    "    del dataset\n",
    "\n",
    "def compute_ensemble_mean(dataset, data_vars):\n",
    "    return [np.mean(dataset[v], axis=0) for v in data_vars]  \n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def extract_patch(data, centers, delta=10):\n",
    "    \"\"\"Extract patches \n",
    "    \n",
    "    data : shape of (v,y,x)\n",
    "    centers  \n",
    "    \"\"\"\n",
    "    # Ensure the centers do not conflict the boundaries. \n",
    "    centers = nb.typed.List(centers)\n",
    "    \n",
    "    patches = [ ]\n",
    "    for obj_y, obj_x in centers:\n",
    "        patches.append( data[:, obj_y-delta:obj_y+delta, obj_x-delta:obj_x+delta] )\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_supercell_probability(dataset, max_size=5, classify_embedded=True):\n",
    "    \"\"\"Compute the neighborhood maximum ensemble probability (NMEP; Schwartz and Sobash 2017) \n",
    "    of a supercell. Supercells are identified using the storm mode classification \n",
    "    scheme from Potvin et al. (2022). \n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    dataset : xarray.dataset \n",
    "        A WoFS ENS summary file containing `comp_dz` and `uh_2to5_instant`.\n",
    "    \n",
    "    max_size : int (default=5)\n",
    "        The maximum filter diameter (in grid points)\n",
    "    \n",
    "    classify_embedded : True/False (default=True)\n",
    "        Setting classify_embedded=False restricts the storm mode classification \n",
    "        to a 3-mode scheme, which runs much faster than the 7-mode scheme. \n",
    "    \n",
    "    References: \n",
    "        Schwartz, C. S. & Sobash, R. A. (2017). Generating probabilistic forecasts from \n",
    "        convection-allowing ensembles using neighborhood approaches: \n",
    "        A review and recommendations. Monthly Weather Review. \n",
    "        https://doi.org/10.1175/mwr-d-16-0400.1\n",
    "        \n",
    "        Potvin, C. K., and co-authors (2022). An Iterative Storm Segmentation \n",
    "        and Classification Algorithm for Convection-Allowing Models and Gridded Radar Analyses,\n",
    "        Journal of Atmospheric and Oceanic Technology, 39(7), 999-1013.\n",
    "    \"\"\"\n",
    "    # Identify supercell regions per ensemble member. \n",
    "    supercell_per_mem = []\n",
    "\n",
    "    for i in range(ENSEMBLE_SIZE):  \n",
    "        dbz_vals = dataset['comp_dz'].values[i,:,:]\n",
    "        rot_vals = dataset['uh_2to5_instant'].values[i,:,:]\n",
    "        clf = monte_python.StormModeClassifier()\n",
    "        # Setting classify_embedded=False, restricts the storm mode classification \n",
    "        # to a 3-mode scheme, which runs much faster than the 7-mode scheme. \n",
    "        storm_modes, labels, dbz_props = clf.classify(dbz_vals, rot_vals, \n",
    "                                                      classify_embedded=classify_embedded)\n",
    "\n",
    "        # We want to isolate the supercells within the domain. \n",
    "        supercell_label = clf.MODES.index('SUPERCELL')+1\n",
    "        supercell_binary = np.where(storm_modes==supercell_label,1,0)\n",
    "\n",
    "        # Apply a maximum value filter to reduce phase errors between members\n",
    "        supercell_binary = maximum_filter(supercell_binary, size=max_size)\n",
    "\n",
    "        supercell_per_mem.append(supercell_binary)\n",
    "\n",
    "    # Compute the ensemble probability of a supercell\n",
    "    supercell_prob = np.mean(supercell_per_mem, axis=0)\n",
    "    \n",
    "    return supercell_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storm_patches(supercell_prob, ds_svr):\n",
    "    \"\"\"Using the supercell NMEP, identify object centers and then \n",
    "    extract the patches. \"\"\"\n",
    "    \n",
    "    params= {'min_thresh': 5,\n",
    "         'max_thresh': 18,\n",
    "         'data_increment':1,\n",
    "         'area_threshold': 500,\n",
    "         'dist_btw_objects': 25} \n",
    "    \n",
    "    input_data = quantize_probabilities(supercell_prob, ENSEMBLE_SIZE)\n",
    "    \n",
    "    sup_labels, sup_props = monte_python.label(input_data = input_data, \n",
    "                       method ='watershed', \n",
    "                       return_object_properties=True, \n",
    "                       params = params\n",
    "                       )\n",
    "\n",
    "    # Use those supercells to center the patches \n",
    "    centers = [region.centroid for region in sup_props] \n",
    "    data = compute_ensemble_mean(ds_svr, SVR_VARIABLES)\n",
    "    data.append(supercell_prob)\n",
    "    data = np.array(data)\n",
    "    patches = np.array(extract_patch(data, centers, delta=PATCH_SIZE_RADIUS))\n",
    "\n",
    "    variables = SVR_VARIABLES + ['supercell probs']\n",
    "    \n",
    "    data = {f'{v}_ens_mean' if v in SVR_VARIABLES else v\n",
    "            : (['n_samples', 'ny', 'nx'], patches[:,i,:,:]) for i,v in enumerate(variables)}\n",
    "\n",
    "    # Convert data to xarray.Dataset.\n",
    "    dataset = xr.Dataset(data)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b8963",
   "metadata": {},
   "source": [
    "## Example Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for a single case. \n",
    "dates = ['20210524', '20210526']\n",
    "init_times = ['2300']\n",
    "time_indices = [12]\n",
    "\n",
    "# TODO: add the storm reports!! \n",
    "\n",
    "def worker_fn(date, init_time, time_index):\n",
    "    \"\"\"A worker function for multiprocessing.\"\"\"\n",
    "\n",
    "    # Load the data. \n",
    "    try:\n",
    "        ds_ens, ds_svr = load_data(date, init_time, time_index)\n",
    "    except:\n",
    "        print(f'Unable to load data for {date}, {init_time}, {time_index}')\n",
    "        return None\n",
    "        \n",
    "    # Compute the probability of a supercell.\n",
    "    supercell_prob = compute_supercell_probability(ds_ens, max_size=5, classify_embedded=True)\n",
    "\n",
    "    # Check that supercell prob exceeds some threshold!\n",
    "    if np.max(supercell_prob) > PROB_THRESH: \n",
    "        # Get data patches. \n",
    "        dataset = get_storm_patches(supercell_prob, ds_svr)\n",
    "    \n",
    "        # Save the data. \n",
    "        save_dataset(join(OUTDIR, f'wofs_data_{date}_{init_time}_{time_index}.nc'), dataset)\n",
    "        \n",
    "if N_JOBS == 1:\n",
    "    args_iterator = to_iterator(dates, init_times, time_indices)   \n",
    "    for date, init_time, time_idx in args_iterator:\n",
    "        print(date, init_time, time_idx)\n",
    "        worker_fn(date, init_time, time_idx)\n",
    "else:\n",
    "    args_iterator = to_iterator(dates, init_times, time_indices) \n",
    "    run_parallel(worker_fn, args_iterator, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot data using built-in plot \n",
    "dataset['supercell probs'][0,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416799bf",
   "metadata": {},
   "source": [
    "### Optional Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d35997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Plot the storm modes and labels to get familiar with the data! \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "dbz_vals = ds_ens['comp_dz'].values[i,:,:]\n",
    "rot_vals = ds_ens['uh_2to5_instant'].values[i,:,:]\n",
    "clf = monte_python.StormModeClassifier()\n",
    "storm_modes, labels, dbz_props = clf.classify(dbz_vals, rot_vals, \n",
    "                                                      classify_embedded=False)\n",
    "\n",
    "x,y = np.meshgrid(range(dbz_vals.shape[0]), range(dbz_vals.shape[1]))\n",
    "fig, axes = plt.subplots(dpi=300, ncols=2, nrows=2, figsize=(8,8))\n",
    "\n",
    "axes[0,0].contourf(x,y,dbz_vals, alpha=0.6, levels=np.arange(20,75,5), cmap='jet')\n",
    "monte_python.plot_storm_labels(x, y, labels, dbz_props, ax=axes[0,1]) \n",
    "monte_python.plot_storm_modes(x, y, storm_modes, dbz_props, clf.converter, ax=axes[1,1]) \n",
    "\n",
    "axes[1,0].contourf(x,y,supercell_prob, alpha=0.6, levels=np.arange(0.1, 1.1, 0.1), cmap='rainbow')\n",
    "monte_python.plot_storm_labels(x, y, sup_labels, sup_props, ax=axes[1,0], alpha=0.6) \n",
    "\n",
    "titles = ['WoFS dBZ', 'Storm Labels', 'Supercell Regions', 'Storm Modes']\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.set_title(titles[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ef409a536f49ada0304313702f2ad53930195e7b40e9dbc3d4056d158a83db7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
