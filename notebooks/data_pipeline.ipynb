{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e1d0bd",
   "metadata": {},
   "source": [
    "## Data Workflow \n",
    "\n",
    "This notebooks contains the basic workflow for processing the WoFS data for identifying confident supercell forecasts and centering patches to extract environmental data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4307ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conda requirement: python 3.8, xarray, numpy, scipy, numba, monte_python, scikit-explain\n",
    "\n",
    "# Import required libraries \n",
    "from glob import glob \n",
    "from os.path import join\n",
    "from datetime import timedelta\n",
    "\n",
    "# Xarray is a great library for processing n-dimensional arrays! \n",
    "import xarray as xr \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numba as nb \n",
    "from scipy.ndimage import maximum_filter\n",
    "from skexplain.common.multiprocessing_utils import run_parallel, to_iterator \n",
    "\n",
    "# Download https://github.com/WarnOnForecast/wofs_ml_severe\n",
    "# change your system path. This script handles loading the \n",
    "# the storm events CSV file and getting the correct reports \n",
    "# for a given time range. \n",
    "import sys\n",
    "sys.path.append('/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "from wofs_ml_severe.data_pipeline.storm_report_loader import StormReportLoader\n",
    "\n",
    "# To use MontePython, clone it to your local directory\n",
    "# and then run `python setup.py install` \n",
    "import monte_python \n",
    "from monte_python.object_identification import quantize_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62efdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the constant variables (like base paths).\n",
    "# Some of these parameters are tunable, so I encourage you to play around with them!\n",
    "\n",
    "BASE_DATA_PATH = '/work/mflora/SummaryFiles/'\n",
    "OUTDIR = '/work/mflora/tmp'\n",
    "ENSEMBLE_SIZE = 18 \n",
    "PATCH_SIZE_RADIUS = 40 # on a 3-km grid, this would be 40 x 40 patch or 120 x 120 km grid. \n",
    "PROB_THRESH = 4/18 \n",
    "N_JOBS = 1\n",
    "SVR_VARIABLES = ['srh_0to1', 'cape_ml', 'cin_ml']\n",
    "\n",
    "STORM_EVENTS_PATH = join(OUTDIR, 'StormEvents_2017-2021.csv')\n",
    "\n",
    "PARAMS = {'min_thresh': 5,\n",
    "         'max_thresh': 18,\n",
    "         'data_increment':1,\n",
    "         'area_threshold': 500,\n",
    "         'dist_btw_objects': 25} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55626cf8",
   "metadata": {},
   "source": [
    "### Functions for loading the WoFS summary files and the observed storm reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac3f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_max(dataset):\n",
    "    return dataset.max(dim='time') \n",
    "\n",
    "def load_wofs_data(date, init_time, time_index):\n",
    "    \"\"\"Load the appropriate WoFS summary files\"\"\"\n",
    "    assert time_index>=12 \n",
    "    # The lead time for the supercell has to be a minimum of 1 hr\n",
    "    # This prevents sampling too close to initialization \n",
    "    # and allow us to consistently sample the enviroment \n",
    "    # at a series of preceding times (e.g., t-45, t-30, t-15, t-60)\n",
    "    file_path_ens = glob(join(BASE_DATA_PATH, date, init_time, f'wofs_ENS_{time_index:02d}*'))[0]\n",
    "    \n",
    "    # Samples the WoFS SVR file every 15 going back an hour. \n",
    "    preceding_time_idxs = np.arange(time_index-12, time_index, 3)\n",
    "    \n",
    "    file_paths_svr = [glob(join(BASE_DATA_PATH, date, init_time, f'wofs_SVR_{t:02d}*'))[0] \n",
    "                     for t in preceding_time_idxs]\n",
    "\n",
    "    ds_ens = xr.load_dataset(file_path_ens, decode_times=False)\n",
    "    \n",
    "    # For supercell identification, we need rotation tracks \n",
    "    # valid over the preceding 30 mins\n",
    "    preceding_time_idxs  = np.arange(time_index-6, time_index+1, 1)\n",
    "    file_paths_ens = [glob(join(BASE_DATA_PATH, date, init_time, f'wofs_ENS_{t:02d}*'))[0] \n",
    "                     for t in preceding_time_idxs]\n",
    "    \n",
    "    ds_svr = xr.concat([xr.load_dataset(f, decode_times=False) \n",
    "                        for f in file_paths_svr], dim='time')\n",
    "    \n",
    "    ds_ens_rot = xr.concat([xr.load_dataset(f, decode_times=False)['uh_2to5_instant'] \n",
    "                        for f in file_paths_ens], dim='time')\n",
    "    \n",
    "    ds_ens['uh_2to5_instant_time_max'] = (['ne', 'lat', 'lon'], compute_time_max(ds_ens_rot).values)\n",
    "    \n",
    "    return ds_ens, ds_svr, file_path_ens\n",
    "\n",
    "\n",
    "def load_reports(ncfile, date, init_time, time_index):\n",
    "    \"\"\"Load observed storm reports. This will load any severe or significant severe reports \n",
    "    for tornadoes, severe wind, and severe hail.\"\"\"\n",
    "    DT = 5 \n",
    "    init_dt = date+init_time\n",
    "    # There will be timing errors in the WoFS, so I've added a 15 \"error window\".\n",
    "    # This will load reports +/- 15 mins of the valid forecast time. \n",
    "    initial_time = (pd.to_datetime(init_dt)+timedelta(minutes=time_index*DT)).strftime('%Y%m%d%H%M')\n",
    "    loader = StormReportLoader(reports_path = STORM_EVENTS_PATH,\n",
    "                               report_type = 'NOAA',\n",
    "                               initial_time = initial_time,\n",
    "                               forecast_length = 0,\n",
    "                               err_window = 15,\n",
    "    )\n",
    "    \n",
    "    ds = xr.open_dataset(ncfile, decode_times=False)\n",
    "    ds_reports = loader.to_grid(dataset=ds)\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    \n",
    "    return ds_reports\n",
    "\n",
    "\n",
    "def save_dataset(fname, dataset):\n",
    "    \"\"\" saves xarray dataset to netcdf using compression \"\"\"\n",
    "    comp = dict(zlib=True, complevel=5)\n",
    "    encoding = {var: comp for var in dataset.data_vars}\n",
    "    #os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    dataset.to_netcdf( path = fname, encoding=encoding )\n",
    "    dataset.close( )\n",
    "    del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03c33f",
   "metadata": {},
   "source": [
    "### Utility functions for computing ensemble statistics and extracting patches from the WoFS data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6a9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_mean(dataset, data_vars):\n",
    "    dataset = dataset[data_vars]\n",
    "    dataset = dataset.mean(dim='ne') \n",
    "    dataset = dataset.rename({n : f'{n}_ens_mean' for n in dataset.data_vars})\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def extract_patch_3d(data, centers, delta=10):\n",
    "    \"\"\"Extract patches \n",
    "    \n",
    "    data : shape of (v,y,x)\n",
    "    centers  \n",
    "    \"\"\"\n",
    "    # Ensure the centers do not conflict the boundaries. \n",
    "    centers = nb.typed.List(centers)\n",
    "    \n",
    "    patches = [ ]\n",
    "    for obj_y, obj_x in centers:\n",
    "        patches.append( data[:,:, obj_y-delta:obj_y+delta, obj_x-delta:obj_x+delta] )\n",
    "\n",
    "    return patches\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def extract_patch_2d(data, centers, delta=10):\n",
    "    \"\"\"Extract patches \n",
    "    \n",
    "    data : shape of (v,y,x)\n",
    "    centers  \n",
    "    \"\"\"\n",
    "    # Ensure the centers do not conflict the boundaries. \n",
    "    centers = nb.typed.List(centers)\n",
    "    \n",
    "    patches = [ ]\n",
    "    for obj_y, obj_x in centers:\n",
    "        patches.append( data[:, obj_y-delta:obj_y+delta, obj_x-delta:obj_x+delta] )\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf04ab",
   "metadata": {},
   "source": [
    "### Computing the ensemble probability of a supercell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ba0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_supercell_probability(dataset, max_size=5, classify_embedded=True):\n",
    "    \"\"\"Compute the neighborhood maximum ensemble probability (NMEP; Schwartz and Sobash 2017) \n",
    "    of a supercell. Supercells are identified using the storm mode classification \n",
    "    scheme from Potvin et al. (2022). \n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    dataset : xarray.dataset \n",
    "        A WoFS ENS summary file containing `comp_dz` and `uh_2to5_instant`.\n",
    "    \n",
    "    max_size : int (default=5)\n",
    "        The maximum filter diameter (in grid points)\n",
    "    \n",
    "    classify_embedded : True/False (default=True)\n",
    "        Setting classify_embedded=False restricts the storm mode classification \n",
    "        to a 3-mode scheme, which runs much faster than the 7-mode scheme. \n",
    "    \n",
    "    References: \n",
    "        Schwartz, C. S. & Sobash, R. A. (2017). Generating probabilistic forecasts from \n",
    "        convection-allowing ensembles using neighborhood approaches: \n",
    "        A review and recommendations. Monthly Weather Review. \n",
    "        https://doi.org/10.1175/mwr-d-16-0400.1\n",
    "        \n",
    "        Potvin, C. K., and co-authors (2022). An Iterative Storm Segmentation \n",
    "        and Classification Algorithm for Convection-Allowing Models and Gridded Radar Analyses,\n",
    "        Journal of Atmospheric and Oceanic Technology, 39(7), 999-1013.\n",
    "    \"\"\"\n",
    "    # Identify supercell regions per ensemble member. \n",
    "    supercell_per_mem = []\n",
    "\n",
    "    for i in range(ENSEMBLE_SIZE):  \n",
    "        dbz_vals = dataset['comp_dz'].values[i,:,:]\n",
    "        rot_vals = dataset['uh_2to5_instant_time_max'].values[i,:,:]\n",
    "        clf = monte_python.StormModeClassifier()\n",
    "        # Setting classify_embedded=False, restricts the storm mode classification \n",
    "        # to a 3-mode scheme, which runs much faster than the 7-mode scheme. \n",
    "        storm_modes, labels, dbz_props = clf.classify(dbz_vals, rot_vals, \n",
    "                                                      classify_embedded=classify_embedded)\n",
    "\n",
    "        # We want to isolate the supercells within the domain. \n",
    "        supercell_label = clf.MODES.index('SUPERCELL')+1\n",
    "        supercell_binary = np.where(storm_modes==supercell_label,1,0)\n",
    "\n",
    "        # Apply a maximum value filter to reduce phase errors between members\n",
    "        supercell_binary = maximum_filter(supercell_binary, size=max_size)\n",
    "\n",
    "        supercell_per_mem.append(supercell_binary)\n",
    "\n",
    "    # Compute the ensemble probability of a supercell\n",
    "    supercell_prob = np.mean(supercell_per_mem, axis=0)\n",
    "    \n",
    "    return supercell_prob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0aaa8",
   "metadata": {},
   "source": [
    "### Locating supercell locations in the WoFS domain using the ensemble probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da9bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_supercell_centers(supercell_prob, params, return_data=False):\n",
    "    \"\"\"Identify supercell centers using the enhanced watershed image segementation \n",
    "    method in MontePython.\"\"\"\n",
    "    input_data = quantize_probabilities(supercell_prob, ENSEMBLE_SIZE)\n",
    "    \n",
    "    sup_labels, sup_props = monte_python.label(input_data = input_data, \n",
    "                       method ='watershed', \n",
    "                       return_object_properties=True, \n",
    "                       params = params\n",
    "                       )\n",
    "\n",
    "    # Use those supercells to center the patches \n",
    "    centers = [region.centroid for region in sup_props] \n",
    "    \n",
    "    if return_data:\n",
    "        return sup_labels, sup_props, input_data\n",
    "    else:\n",
    "        return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b5c58",
   "metadata": {},
   "source": [
    "### Extracting the supercell-centered data and creating an xarray.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1387ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storm_patches(centers, ds_svr, ds_reports, supercell_prob):\n",
    "    \"\"\"Using the supercell NMEP, identify object centers and then \n",
    "    extract the patches. \"\"\"\n",
    "    # (Optional): This initial script just computes the ensemble mean \n",
    "    # environmental field. In the future, you may want to expand compute\n",
    "    # other statistics. \n",
    "    dataset = compute_ensemble_mean(ds_svr, SVR_VARIABLES)\n",
    "    new_variables = list(dataset.data_vars)\n",
    "    \n",
    "    # Add the supercell probabilities to the dataset. \n",
    "    dataset['supercell probs'] = (['NY', 'NX'], supercell_prob)\n",
    "    \n",
    "    # Append the reports\n",
    "    ds_reports_vars = list(ds_reports.data_vars)\n",
    "    for v in ds_reports_vars:\n",
    "        dataset[v] = (['NY', 'NX'], ds_reports[v].values)\n",
    "    \n",
    "    variables_3d = new_variables\n",
    "    variables_2d = [v for v in dataset.data_vars if v not in variables_3d]\n",
    "    \n",
    "    variables = list(dataset.data_vars)\n",
    "    data_3d = np.array([dataset[v].values for v in variables_3d])\n",
    "    data_2d = np.array([dataset[v].values for v in variables_2d])\n",
    "    \n",
    "    patches_2d = np.array(extract_patch_2d(data_2d, centers, delta=PATCH_SIZE_RADIUS))\n",
    "    patches_3d = np.array(extract_patch_3d(data_3d, centers, delta=PATCH_SIZE_RADIUS))\n",
    "\n",
    "    dict_2d = {v : (['n_samples', 'ny', 'nx'], patches_2d[:,i,:,:]) for i,v in enumerate(variables_2d)}\n",
    "    dict_3d = {v : (['n_samples', 'nt', 'ny', 'nx'], patches_3d[:,:,i,:,:]) for i,v in enumerate(variables_3d)}\n",
    "    \n",
    "    data = {**dict_2d, **dict_3d}\n",
    "    \n",
    "    # Convert data to xarray.Dataset.\n",
    "    dataset = xr.Dataset(data)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b8963",
   "metadata": {},
   "source": [
    "## Example Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39935434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20210524 2000 12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (7,80,80) into shape (7,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date, init_time, time_idx \u001b[38;5;129;01min\u001b[39;00m args_iterator:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(date, init_time, time_idx)\n\u001b[0;32m---> 44\u001b[0m         \u001b[43mworker_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     args_iterator \u001b[38;5;241m=\u001b[39m to_iterator(dates, init_times, time_indices) \n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mworker_fn\u001b[0;34m(date, init_time, time_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m centers \u001b[38;5;241m=\u001b[39m identify_supercell_centers(supercell_prob, PARAMS)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Get data patches. \u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_storm_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_svr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_reports\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupercell_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Save the data. \u001b[39;00m\n\u001b[1;32m     38\u001b[0m save_dataset(join(OUTDIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwofs_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m), dataset)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mget_storm_patches\u001b[0;34m(centers, ds_svr, ds_reports, supercell_prob)\u001b[0m\n\u001b[1;32m     22\u001b[0m data_3d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dataset[v]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables_3d])\n\u001b[1;32m     23\u001b[0m data_2d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([dataset[v]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables_2d])\n\u001b[0;32m---> 25\u001b[0m patches_2d \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_patch_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATCH_SIZE_RADIUS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m patches_3d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(extract_patch_3d(data_3d, centers, delta\u001b[38;5;241m=\u001b[39mPATCH_SIZE_RADIUS))\n\u001b[1;32m     28\u001b[0m dict_2d \u001b[38;5;241m=\u001b[39m {v : ([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_samples\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mny\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnx\u001b[39m\u001b[38;5;124m'\u001b[39m], patches_2d[:,i,:,:]) \u001b[38;5;28;01mfor\u001b[39;00m i,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(variables_2d)}\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (7,80,80) into shape (7,)"
     ]
    }
   ],
   "source": [
    "# You'll create a list of cases, init times. At the moment, time indices should start at 12 (=60 min forecast).\n",
    "# In the future, you can explore longer lead times!\n",
    "dates = ['20210524']# '20210526']\n",
    "init_times = ['2000']\n",
    "time_indices = [12]\n",
    "\n",
    "# TODO: add the storm reports!! \n",
    "\n",
    "def worker_fn(date, init_time, time_index):\n",
    "    \"\"\"A worker function for multiprocessing.\"\"\"\n",
    "\n",
    "    # Load the data. \n",
    "    #try:\n",
    "    ds_ens, ds_svr, ens_file = load_wofs_data(date, init_time, time_index)\n",
    "    ds_reports = load_reports(ens_file, date, init_time, time_index)\n",
    "        \n",
    "    #except:\n",
    "    #    print(f'Unable to load data for {date}, {init_time}, {time_index}')\n",
    "    #    return None\n",
    "        \n",
    "    # Compute the probability of a supercell. \n",
    "    # max_size and classify_embedded are changable parameters. I initially picked \n",
    "    # a larger max_size to increase the odds of identifying confident supercell \n",
    "    # forecasts. I classify_embedded = True to also increase the odds of identify\n",
    "    # potentially embedded supercells. \n",
    "    supercell_prob = compute_supercell_probability(ds_ens, max_size=5, classify_embedded=True)\n",
    "\n",
    "    # Check that supercell prob exceeds some threshold!\n",
    "    if np.max(supercell_prob) > PROB_THRESH: \n",
    "    \n",
    "        # Use those supercells to center the patches \n",
    "        centers = identify_supercell_centers(supercell_prob, PARAMS)\n",
    "    \n",
    "        # Get data patches. \n",
    "        dataset = get_storm_patches(centers, ds_svr, ds_reports, supercell_prob)\n",
    "    \n",
    "        # Save the data. \n",
    "        save_dataset(join(OUTDIR, f'wofs_data_{date}_{init_time}_{time_index}.nc'), dataset)\n",
    "        \n",
    "if N_JOBS == 1:\n",
    "    args_iterator = to_iterator(dates, init_times, time_indices)   \n",
    "    for date, init_time, time_idx in args_iterator:\n",
    "        print(date, init_time, time_idx)\n",
    "        worker_fn(date, init_time, time_idx)\n",
    "else:\n",
    "    args_iterator = to_iterator(dates, init_times, time_indices) \n",
    "    run_parallel(worker_fn, args_iterator, n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416799bf",
   "metadata": {},
   "source": [
    "### (Optional) Plotting the storm patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb019e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot data using built-in plot. In this example, I've plotted \n",
    "# the supercell ensemble probabilities (which is what the patches are centered on),\n",
    "# any tornado reports, and the ensemble mean MLCIN and MLCAPE (@ t-60), respectively.\n",
    "\n",
    "# You'll have to load the datasets to use this code! \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# You'll have to load the datasets to use this code! \n",
    "dataset = xr.load_dataset(join(OUTDIR, 'wofs_data_20210524_2000_12.nc'))\n",
    "\n",
    "fig, axes = plt.subplots(dpi=300, nrows=2, ncols=2)\n",
    "\n",
    "variables = ['supercell probs', 'tornado_severe', 'cin_ml_ens_mean', 'cape_ml_ens_mean']\n",
    "dims = ['2d', '2d', '3d', '3d']\n",
    "titles = ['Supercell NMEP', 'Tornado Reports', 'Ens. Mean MLCIN', 'Ens. Mean MLCAPE']\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if dims[i] == '2d':\n",
    "        dataset[variables[i]][0,:,:].plot(ax=ax)\n",
    "    else:\n",
    "        dataset[variables[i]][0,0,:,:].plot(ax=ax)\n",
    "    ax.set_title(titles[i])  \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffd91b",
   "metadata": {},
   "source": [
    "### (Optional) Plotting the different components of the data workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d35997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Plot the storm modes and labels to get familiar with the data! \n",
    "date = '20210524'; init_time = '2000'; time_index = 12\n",
    "\n",
    "# This is only being shown for a single ensemble member!!!\n",
    "\n",
    "i = 0\n",
    "file_path_ens = glob(join(BASE_DATA_PATH, date, init_time, f'wofs_ENS_{time_index:02d}*'))[0]\n",
    "ds_ens = xr.load_dataset(file_path_ens, decode_times=False)\n",
    "\n",
    "dbz_vals = ds_ens['comp_dz'].values[i,:,:]\n",
    "rot_vals = ds_ens['uh_2to5_instant'].values[i,:,:]\n",
    "clf = monte_python.StormModeClassifier()\n",
    "storm_modes, labels, dbz_props = clf.classify(dbz_vals, rot_vals, \n",
    "                                                      classify_embedded=False)\n",
    "\n",
    "supercell_prob = compute_supercell_probability(ds_ens, max_size=5, classify_embedded=False)\n",
    "sup_labels, sup_props, _ = identify_supercell_centers(supercell_prob, PARAMS, return_data=True)\n",
    "\n",
    "x,y = np.meshgrid(range(dbz_vals.shape[0]), range(dbz_vals.shape[1]))\n",
    "fig, axes = plt.subplots(dpi=300, ncols=2, nrows=2, figsize=(8,8))\n",
    "\n",
    "axes[0,0].contourf(x,y,dbz_vals, alpha=0.6, levels=np.arange(20,75,5), cmap='jet')\n",
    "monte_python.plot_storm_labels(x, y, labels, dbz_props, ax=axes[0,1]) \n",
    "monte_python.plot_storm_modes(x, y, storm_modes, dbz_props, clf.converter, ax=axes[1,1]) \n",
    "\n",
    "axes[1,0].contourf(x,y,supercell_prob, alpha=0.6, levels=np.arange(0.1, 1.1, 0.1), cmap='rainbow')\n",
    "monte_python.plot_storm_labels(x, y, sup_labels, sup_props, ax=axes[1,0], alpha=0.6) \n",
    "\n",
    "titles = ['WoFS dBZ', 'Storm Labels', 'Supercell Regions', 'Storm Modes']\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.set_title(titles[i])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ef409a536f49ada0304313702f2ad53930195e7b40e9dbc3d4056d158a83db7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
